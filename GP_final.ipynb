{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245ada36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d93f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gpytorch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3e25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef82055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_data(train_file):\n",
    "    col_names = ['unit', 'cycle'] + \\\n",
    "                [f'op_setting_{i}' for i in range(1, 4)] + \\\n",
    "                [f'sensor_{i}' for i in range(1, 22)]\n",
    "    df = pd.read_csv(train_file, delim_whitespace=True, header=None, names=col_names)\n",
    "    max_cycle = df.groupby('unit')['cycle'].max().reset_index()\n",
    "    max_cycle.columns = ['unit', 'max_cycle']\n",
    "    df = df.merge(max_cycle, on='unit', how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['cycle']\n",
    "    return df\n",
    "\n",
    "# 2) Preprocessing (features + scaled target + raw RUL)\n",
    "def preprocess(df, unit_ids=None):\n",
    "    if unit_ids is not None:\n",
    "        df = df[df['unit'].isin(unit_ids)]\n",
    "    raw_rul = df['RUL'].values  # save true RUL\n",
    "    \n",
    "    df = df.drop(columns=['unit', 'cycle', 'max_cycle', 'RUL'])\n",
    "    # Normalize features to [-1, 1] range\n",
    "    \n",
    "    X = df.values\n",
    "    y = raw_rul.reshape(-1, 1)\n",
    "    fscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X = fscaler.fit_transform(X)\n",
    "    # y = tscaler.fit_transform(y)\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).flatten().float()\n",
    "    return X, y, raw_rul, fscaler, tscaler\n",
    "\n",
    "#  Kernel factory\n",
    "def get_kernel(kernel_type, **kwargs):\n",
    " \n",
    " \n",
    "# Base kernels\n",
    "    if kernel_type == 'RBF':\n",
    "        base = gpytorch.kernels.RBFKernel(**kwargs)\n",
    "    elif kernel_type == 'Matern':\n",
    "        base = gpytorch.kernels.MaternKernel(**kwargs)  # default nu=2.5\n",
    "    elif kernel_type == 'Mat32':\n",
    "        base = gpytorch.kernels.MaternKernel(nu=1.5, **kwargs)\n",
    "    elif kernel_type == 'Mat52':\n",
    "        base = gpytorch.kernels.MaternKernel(nu=2.5, **kwargs)\n",
    "    elif kernel_type == 'Linear':\n",
    "        base = gpytorch.kernels.LinearKernel(**kwargs)\n",
    "    elif kernel_type == 'Periodic':\n",
    "        base = gpytorch.kernels.PeriodicKernel(**kwargs)\n",
    "    elif kernel_type == 'RQ':\n",
    "        base = gpytorch.kernels.RQKernel(**kwargs)\n",
    "    elif kernel_type == 'Poly':\n",
    "        base = gpytorch.kernels.PolynomialKernel(**kwargs)\n",
    "    elif kernel_type == 'SpectralMixture':\n",
    "        # e.g. num_mixtures=4, ard_num_dims=X.shape[1]\n",
    "        nm = kwargs.pop('num_mixtures', 4)\n",
    "        base = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=nm, **kwargs)\n",
    "\n",
    "    # Composite kernels\n",
    "    elif kernel_type == 'RBF_Linear':\n",
    "        base = gpytorch.kernels.RBFKernel(**kwargs) + gpytorch.kernels.LinearKernel(**kwargs)\n",
    "\n",
    "    elif kernel_type == 'Matern_Linear':\n",
    "        base = gpytorch.kernels.MaternKernel(**kwargs) + gpytorch.kernels.LinearKernel(**kwargs)\n",
    "    elif kernel_type == 'Mat52_Linear':\n",
    "        base = (\n",
    "            gpytorch.kernels.MaternKernel(nu=2.5, **kwargs)\n",
    "            + gpytorch.kernels.LinearKernel(**kwargs)\n",
    "        )\n",
    "    elif kernel_type == 'RQ':\n",
    "        base = gpytorch.kernels.RQKernel(**kwargs)\n",
    "    elif kernel_type == 'RQ_Linear':\n",
    "        base = gpytorch.kernels.RQKernel(**kwargs) + gpytorch.kernels.LinearKernel(**kwargs)\n",
    "    elif kernel_type == 'Periodic_RBF':\n",
    "        base = gpytorch.kernels.PeriodicKernel(**kwargs) + gpytorch.kernels.RBFKernel(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kernel: {kernel_type}\")\n",
    "\n",
    "    # Wrap in a ScaleKernel so we learn an output-scale\n",
    "    return gpytorch.kernels.ScaleKernel(base)\n",
    "\n",
    "\n",
    "\n",
    "# Now preprocess just this single-sensor dataset\n",
    "def preprocess_single_sensor(df, sensor_col, unit_ids=None):\n",
    "\n",
    "    raw_rul = df['RUL'].values\n",
    "    X = df[[sensor_col]].values\n",
    "    y = raw_rul.reshape(-1, 1)\n",
    "    fscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_scaled = fscaler.fit_transform(X)\n",
    "    # y_scaled = tscaler.fit_transform(y)\n",
    "    return (\n",
    "        torch.from_numpy(X_scaled).float(),\n",
    "        torch.from_numpy(y).flatten().float(),\n",
    "        raw_rul,\n",
    "        fscaler,\n",
    "        tscaler\n",
    "    )\n",
    "    \n",
    "    \n",
    "# Test train split function\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "    indices = torch.randperm(X.size(0))\n",
    "    split_idx = int(X.size(0) * (1 - test_size))\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "    \n",
    "    train_x = X[train_indices]\n",
    "    train_y = y[train_indices]\n",
    "    test_x = X[test_indices]\n",
    "    test_y = y[test_indices]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#_________________________GP Functions_______________________\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# def train_model(model, likelihood, train_x, train_y, num_epochs=50):\n",
    "#     # Find optimal model hyperparameters\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "\n",
    "#     # Use the adam optimizer\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "#     # \"Loss\" for GPs - the marginal log likelihood\n",
    "#     mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "#     # training_iter = 50\n",
    "#     for i in range(num_epochs):\n",
    "#         # Zero gradients from previous iteration\n",
    "#         optimizer.zero_grad()\n",
    "#         # Output from model\n",
    "#         output = model(train_x)\n",
    "#         # Calc loss and backprop gradients\n",
    "#         loss = -mll(output, train_y)\n",
    "#         loss.backward()\n",
    "#         print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "#             i + 1, num_epochs, loss.item(),\n",
    "#             model.covar_module.base_kernel.lengthscale.item(),\n",
    "#             model.likelihood.noise.item()\n",
    "#         ))\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     return model, likelihood\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4dc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, likelihood, train_x, train_y, num_epochs=50):\n",
    "    loss = []\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iter {epoch}/{num_epochs} - Loss: {loss.item():.3f}\")\n",
    "    return model, likelihood\n",
    "\n",
    "# Test the model with the test set\n",
    "def test_model(model, likelihood, test_x, test_y):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    # Get the mean and variance of the predictions\n",
    "    pred_mean = observed_pred.mean\n",
    "    pred_var = observed_pred.variance\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = torch.sqrt(torch.mean((pred_mean - test_y) ** 2)).item()\n",
    "    mean_absolute_error = torch.mean(torch.abs(pred_mean - test_y)).item()\n",
    "    \n",
    "    return pred_mean.cpu().numpy(), pred_var.cpu().numpy(), rmse, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf02acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns: ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lindy\\AppData\\Local\\Temp\\ipykernel_24940\\3132579775.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(train_file, delim_whitespace=True, header=None, names=col_names)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "train_file     = \"train_FD001.csv\"  # PHM08 train file\n",
    "kernel_chosen = 'RBF'  # Choose your kernel here\n",
    "\n",
    "# Load data\n",
    "df = load_data(train_file)\n",
    "\n",
    "# Drop bad sensors\n",
    "drop_sensors = [1, 5, 6, 10, 16, 18, 19]\n",
    "drop_cols = [f\"op_setting_{i+1}\" for i in range(3) ] + [f\"sensor_{sensor}\"for sensor in drop_sensors]\n",
    "print(f\"Dropping columns: {drop_cols}\")\n",
    "df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# Preprocess the data\n",
    "sensors_normalized, RuL, _, _, _ = preprocess(df)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_x, train_y, test_x, test_y = train_test_split(sensors_normalized, RuL, test_size=0.2, random_state=42)\n",
    "\n",
    "# data is ready to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90a1715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Moved to GPU\n",
      "Training with kernel: RBF\n",
      "Iter 1/100 - Loss: 1203.511\n",
      "Iter 2/100 - Loss: 1115.328\n",
      "Iter 3/100 - Loss: 1034.189\n",
      "Iter 4/100 - Loss: 960.330\n",
      "Iter 5/100 - Loss: 892.835\n",
      "Iter 6/100 - Loss: 830.063\n",
      "Iter 7/100 - Loss: 773.843\n",
      "Iter 8/100 - Loss: 722.938\n",
      "Iter 9/100 - Loss: 676.764\n",
      "Iter 10/100 - Loss: 637.352\n",
      "Iter 11/100 - Loss: 605.716\n",
      "Iter 12/100 - Loss: 580.794\n",
      "Iter 13/100 - Loss: 561.397\n",
      "Iter 14/100 - Loss: 540.096\n",
      "Iter 15/100 - Loss: 516.504\n",
      "Iter 16/100 - Loss: 492.408\n",
      "Iter 17/100 - Loss: 469.817\n",
      "Iter 18/100 - Loss: 450.269\n",
      "Iter 19/100 - Loss: 433.899\n",
      "Iter 20/100 - Loss: 419.701\n",
      "Iter 21/100 - Loss: 407.072\n",
      "Iter 22/100 - Loss: 395.851\n",
      "Iter 23/100 - Loss: 385.324\n",
      "Iter 24/100 - Loss: 375.206\n",
      "Iter 25/100 - Loss: 365.616\n",
      "Iter 26/100 - Loss: 356.416\n",
      "Iter 27/100 - Loss: 347.700\n",
      "Iter 28/100 - Loss: 339.261\n",
      "Iter 29/100 - Loss: 331.498\n",
      "Iter 30/100 - Loss: 324.401\n",
      "Iter 31/100 - Loss: 317.767\n",
      "Iter 32/100 - Loss: 311.885\n",
      "Iter 33/100 - Loss: 306.829\n",
      "Iter 34/100 - Loss: 302.506\n",
      "Iter 35/100 - Loss: 297.804\n",
      "Iter 36/100 - Loss: 293.231\n",
      "Iter 37/100 - Loss: 288.470\n",
      "Iter 38/100 - Loss: 283.891\n",
      "Iter 39/100 - Loss: 279.424\n",
      "Iter 40/100 - Loss: 275.058\n",
      "Iter 41/100 - Loss: 270.918\n",
      "Iter 42/100 - Loss: 266.849\n",
      "Iter 43/100 - Loss: 263.195\n",
      "Iter 44/100 - Loss: 260.139\n",
      "Iter 45/100 - Loss: 256.830\n",
      "Iter 46/100 - Loss: 253.856\n",
      "Iter 47/100 - Loss: 251.082\n",
      "Iter 48/100 - Loss: 248.251\n",
      "Iter 49/100 - Loss: 245.596\n",
      "Iter 50/100 - Loss: 243.367\n",
      "Iter 51/100 - Loss: 240.878\n",
      "Iter 52/100 - Loss: 238.606\n",
      "Iter 53/100 - Loss: 236.369\n",
      "Iter 54/100 - Loss: 234.269\n",
      "Iter 55/100 - Loss: 231.960\n",
      "Iter 56/100 - Loss: 230.286\n",
      "Iter 57/100 - Loss: 228.312\n",
      "Iter 58/100 - Loss: 226.246\n",
      "Iter 59/100 - Loss: 224.347\n",
      "Iter 60/100 - Loss: 222.453\n",
      "Iter 61/100 - Loss: 220.137\n",
      "Iter 62/100 - Loss: 218.177\n",
      "Iter 63/100 - Loss: 217.012\n",
      "Iter 64/100 - Loss: 215.337\n",
      "Iter 65/100 - Loss: 213.638\n",
      "Iter 66/100 - Loss: 212.082\n",
      "Iter 67/100 - Loss: 210.511\n",
      "Iter 68/100 - Loss: 209.092\n",
      "Iter 69/100 - Loss: 207.611\n",
      "Iter 70/100 - Loss: 206.175\n",
      "Iter 71/100 - Loss: 204.281\n",
      "Iter 72/100 - Loss: 202.927\n",
      "Iter 73/100 - Loss: 202.235\n",
      "Iter 74/100 - Loss: 200.917\n",
      "Iter 75/100 - Loss: 199.659\n",
      "Iter 76/100 - Loss: 198.377\n",
      "Iter 77/100 - Loss: 197.190\n",
      "Iter 78/100 - Loss: 195.920\n",
      "Iter 79/100 - Loss: 194.765\n",
      "Iter 80/100 - Loss: 193.603\n",
      "Iter 81/100 - Loss: 192.366\n",
      "Iter 82/100 - Loss: 191.261\n",
      "Iter 83/100 - Loss: 190.116\n",
      "Iter 84/100 - Loss: 189.054\n",
      "Iter 85/100 - Loss: 187.951\n",
      "Iter 86/100 - Loss: 186.957\n",
      "Iter 87/100 - Loss: 185.927\n",
      "Iter 88/100 - Loss: 184.852\n",
      "Iter 89/100 - Loss: 183.896\n",
      "Iter 90/100 - Loss: 182.874\n",
      "Iter 91/100 - Loss: 182.158\n",
      "Iter 92/100 - Loss: 180.825\n",
      "Iter 93/100 - Loss: 180.091\n",
      "Iter 94/100 - Loss: 179.112\n",
      "Iter 95/100 - Loss: 178.227\n",
      "Iter 96/100 - Loss: 177.355\n",
      "Iter 97/100 - Loss: 176.417\n",
      "Iter 98/100 - Loss: 175.582\n",
      "Iter 99/100 - Loss: 174.687\n",
      "Iter 100/100 - Loss: 173.855\n",
      "Test RMSE: 45.9151\n",
      "Test Mean Absolute Error: 32.1825\n",
      "Training with kernel: Matern\n",
      "Iter 1/100 - Loss: 226.813\n",
      "Iter 2/100 - Loss: 220.667\n",
      "Iter 3/100 - Loss: 214.941\n",
      "Iter 4/100 - Loss: 209.461\n",
      "Iter 5/100 - Loss: 204.205\n",
      "Iter 6/100 - Loss: 199.172\n",
      "Iter 7/100 - Loss: 194.359\n",
      "Iter 8/100 - Loss: 189.760\n",
      "Iter 9/100 - Loss: 185.377\n",
      "Iter 10/100 - Loss: 181.197\n",
      "Iter 11/100 - Loss: 177.236\n",
      "Iter 12/100 - Loss: 173.440\n",
      "Iter 13/100 - Loss: 169.785\n",
      "Iter 14/100 - Loss: 166.274\n",
      "Iter 15/100 - Loss: 162.923\n",
      "Iter 16/100 - Loss: 159.730\n",
      "Iter 17/100 - Loss: 156.673\n",
      "Iter 18/100 - Loss: 153.733\n",
      "Iter 19/100 - Loss: 150.903\n",
      "Iter 20/100 - Loss: 148.188\n",
      "Iter 21/100 - Loss: 145.600\n",
      "Iter 22/100 - Loss: 143.123\n",
      "Iter 23/100 - Loss: 140.733\n",
      "Iter 24/100 - Loss: 138.422\n",
      "Iter 25/100 - Loss: 136.205\n",
      "Iter 26/100 - Loss: 134.090\n",
      "Iter 27/100 - Loss: 132.055\n",
      "Iter 28/100 - Loss: 130.088\n",
      "Iter 29/100 - Loss: 128.182\n",
      "Iter 30/100 - Loss: 126.370\n",
      "Iter 31/100 - Loss: 124.637\n",
      "Iter 32/100 - Loss: 122.951\n",
      "Iter 33/100 - Loss: 121.308\n",
      "Iter 34/100 - Loss: 119.751\n",
      "Iter 35/100 - Loss: 118.245\n",
      "Iter 36/100 - Loss: 116.773\n",
      "Iter 37/100 - Loss: 115.360\n",
      "Iter 38/100 - Loss: 113.999\n",
      "Iter 39/100 - Loss: 112.684\n",
      "Iter 40/100 - Loss: 111.412\n",
      "Iter 41/100 - Loss: 110.181\n",
      "Iter 42/100 - Loss: 109.005\n",
      "Iter 43/100 - Loss: 107.855\n",
      "Iter 44/100 - Loss: 106.746\n",
      "Iter 45/100 - Loss: 105.686\n",
      "Iter 46/100 - Loss: 104.626\n",
      "Iter 47/100 - Loss: 103.608\n",
      "Iter 48/100 - Loss: 102.628\n",
      "Iter 49/100 - Loss: 101.673\n",
      "Iter 50/100 - Loss: 100.740\n",
      "Iter 51/100 - Loss: 99.839\n",
      "Iter 52/100 - Loss: 98.965\n",
      "Iter 53/100 - Loss: 98.110\n",
      "Iter 54/100 - Loss: 97.284\n",
      "Iter 55/100 - Loss: 96.485\n",
      "Iter 56/100 - Loss: 95.708\n",
      "Iter 57/100 - Loss: 94.948\n",
      "Iter 58/100 - Loss: 94.201\n",
      "Iter 59/100 - Loss: 93.477\n",
      "Iter 60/100 - Loss: 92.758\n",
      "Iter 61/100 - Loss: 92.076\n",
      "Iter 62/100 - Loss: 91.399\n",
      "Iter 63/100 - Loss: 90.736\n",
      "Iter 64/100 - Loss: 90.104\n",
      "Iter 65/100 - Loss: 89.473\n",
      "Iter 66/100 - Loss: 88.855\n",
      "Iter 67/100 - Loss: 88.259\n",
      "Iter 68/100 - Loss: 87.674\n",
      "Iter 69/100 - Loss: 87.092\n",
      "Iter 70/100 - Loss: 86.532\n",
      "Iter 71/100 - Loss: 85.985\n",
      "Iter 72/100 - Loss: 85.443\n",
      "Iter 73/100 - Loss: 84.913\n",
      "Iter 74/100 - Loss: 84.409\n",
      "Iter 75/100 - Loss: 83.891\n",
      "Iter 76/100 - Loss: 83.399\n",
      "Iter 77/100 - Loss: 82.912\n",
      "Iter 78/100 - Loss: 82.429\n",
      "Iter 79/100 - Loss: 81.958\n",
      "Iter 80/100 - Loss: 81.504\n",
      "Iter 81/100 - Loss: 81.052\n",
      "Iter 82/100 - Loss: 80.600\n",
      "Iter 83/100 - Loss: 80.164\n",
      "Iter 84/100 - Loss: 79.740\n",
      "Iter 85/100 - Loss: 79.309\n",
      "Iter 86/100 - Loss: 78.894\n",
      "Iter 87/100 - Loss: 78.488\n",
      "Iter 88/100 - Loss: 78.089\n",
      "Iter 89/100 - Loss: 77.699\n",
      "Iter 90/100 - Loss: 77.309\n",
      "Iter 91/100 - Loss: 76.928\n",
      "Iter 92/100 - Loss: 76.558\n",
      "Iter 93/100 - Loss: 76.201\n",
      "Iter 94/100 - Loss: 75.839\n",
      "Iter 95/100 - Loss: 75.466\n",
      "Iter 96/100 - Loss: 75.109\n",
      "Iter 97/100 - Loss: 74.768\n",
      "Iter 98/100 - Loss: 74.404\n",
      "Iter 99/100 - Loss: 74.078\n",
      "Iter 100/100 - Loss: 73.725\n",
      "Test RMSE: 43.0338\n",
      "Test Mean Absolute Error: 30.1534\n",
      "Training with kernel: Linear\n",
      "Iter 1/100 - Loss: 110.558\n",
      "Iter 2/100 - Loss: 109.331\n",
      "Iter 3/100 - Loss: 108.142\n",
      "Iter 4/100 - Loss: 106.990\n",
      "Iter 5/100 - Loss: 105.871\n",
      "Iter 6/100 - Loss: 104.784\n",
      "Iter 7/100 - Loss: 103.727\n",
      "Iter 8/100 - Loss: 102.698\n",
      "Iter 9/100 - Loss: 101.697\n",
      "Iter 10/100 - Loss: 100.721\n",
      "Iter 11/100 - Loss: 99.770\n",
      "Iter 12/100 - Loss: 98.843\n",
      "Iter 13/100 - Loss: 97.938\n",
      "Iter 14/100 - Loss: 97.056\n",
      "Iter 15/100 - Loss: 96.195\n",
      "Iter 16/100 - Loss: 95.354\n",
      "Iter 17/100 - Loss: 94.533\n",
      "Iter 18/100 - Loss: 93.732\n",
      "Iter 19/100 - Loss: 92.949\n",
      "Iter 20/100 - Loss: 92.185\n",
      "Iter 21/100 - Loss: 91.438\n",
      "Iter 22/100 - Loss: 90.708\n",
      "Iter 23/100 - Loss: 89.994\n",
      "Iter 24/100 - Loss: 89.297\n",
      "Iter 25/100 - Loss: 88.616\n",
      "Iter 26/100 - Loss: 87.949\n",
      "Iter 27/100 - Loss: 87.298\n",
      "Iter 28/100 - Loss: 86.660\n",
      "Iter 29/100 - Loss: 86.037\n",
      "Iter 30/100 - Loss: 85.427\n",
      "Iter 31/100 - Loss: 84.830\n",
      "Iter 32/100 - Loss: 84.246\n",
      "Iter 33/100 - Loss: 83.674\n",
      "Iter 34/100 - Loss: 83.114\n",
      "Iter 35/100 - Loss: 82.566\n",
      "Iter 36/100 - Loss: 82.029\n",
      "Iter 37/100 - Loss: 81.503\n",
      "Iter 38/100 - Loss: 80.987\n",
      "Iter 39/100 - Loss: 80.482\n",
      "Iter 40/100 - Loss: 79.987\n",
      "Iter 41/100 - Loss: 79.501\n",
      "Iter 42/100 - Loss: 79.025\n",
      "Iter 43/100 - Loss: 78.558\n",
      "Iter 44/100 - Loss: 78.100\n",
      "Iter 45/100 - Loss: 77.651\n",
      "Iter 46/100 - Loss: 77.210\n",
      "Iter 47/100 - Loss: 76.777\n",
      "Iter 48/100 - Loss: 76.352\n",
      "Iter 49/100 - Loss: 75.934\n",
      "Iter 50/100 - Loss: 75.524\n",
      "Iter 51/100 - Loss: 75.122\n",
      "Iter 52/100 - Loss: 74.726\n",
      "Iter 53/100 - Loss: 74.337\n",
      "Iter 54/100 - Loss: 73.955\n",
      "Iter 55/100 - Loss: 73.579\n",
      "Iter 56/100 - Loss: 73.209\n",
      "Iter 57/100 - Loss: 72.846\n",
      "Iter 58/100 - Loss: 72.488\n",
      "Iter 59/100 - Loss: 72.136\n",
      "Iter 60/100 - Loss: 71.790\n",
      "Iter 61/100 - Loss: 71.449\n",
      "Iter 62/100 - Loss: 71.114\n",
      "Iter 63/100 - Loss: 70.784\n",
      "Iter 64/100 - Loss: 70.458\n",
      "Iter 65/100 - Loss: 70.138\n",
      "Iter 66/100 - Loss: 69.823\n",
      "Iter 67/100 - Loss: 69.512\n",
      "Iter 68/100 - Loss: 69.205\n",
      "Iter 69/100 - Loss: 68.904\n",
      "Iter 70/100 - Loss: 68.606\n",
      "Iter 71/100 - Loss: 68.313\n",
      "Iter 72/100 - Loss: 68.023\n",
      "Iter 73/100 - Loss: 67.738\n",
      "Iter 74/100 - Loss: 67.457\n",
      "Iter 75/100 - Loss: 67.179\n",
      "Iter 76/100 - Loss: 66.906\n",
      "Iter 77/100 - Loss: 66.635\n",
      "Iter 78/100 - Loss: 66.369\n",
      "Iter 79/100 - Loss: 66.106\n",
      "Iter 80/100 - Loss: 65.846\n",
      "Iter 81/100 - Loss: 65.590\n",
      "Iter 82/100 - Loss: 65.337\n",
      "Iter 83/100 - Loss: 65.087\n",
      "Iter 84/100 - Loss: 64.840\n",
      "Iter 85/100 - Loss: 64.596\n",
      "Iter 86/100 - Loss: 64.356\n",
      "Iter 87/100 - Loss: 64.118\n",
      "Iter 88/100 - Loss: 63.883\n",
      "Iter 89/100 - Loss: 63.651\n",
      "Iter 90/100 - Loss: 63.421\n",
      "Iter 91/100 - Loss: 63.194\n",
      "Iter 92/100 - Loss: 62.970\n",
      "Iter 93/100 - Loss: 62.749\n",
      "Iter 94/100 - Loss: 62.530\n",
      "Iter 95/100 - Loss: 62.313\n",
      "Iter 96/100 - Loss: 62.099\n",
      "Iter 97/100 - Loss: 61.887\n",
      "Iter 98/100 - Loss: 61.678\n",
      "Iter 99/100 - Loss: 61.470\n",
      "Iter 100/100 - Loss: 61.265\n",
      "Test RMSE: 45.0057\n",
      "Test Mean Absolute Error: 34.2502\n",
      "Training with kernel: RBF_Linear\n",
      "Iter 1/100 - Loss: 53.477\n",
      "Iter 2/100 - Loss: 53.006\n",
      "Iter 3/100 - Loss: 52.562\n",
      "Iter 4/100 - Loss: 52.139\n",
      "Iter 5/100 - Loss: 51.734\n",
      "Iter 6/100 - Loss: 51.343\n",
      "Iter 7/100 - Loss: 50.964\n",
      "Iter 8/100 - Loss: 50.595\n",
      "Iter 9/100 - Loss: 50.233\n",
      "Iter 10/100 - Loss: 49.877\n",
      "Iter 11/100 - Loss: 49.525\n",
      "Iter 12/100 - Loss: 49.180\n",
      "Iter 13/100 - Loss: 48.843\n",
      "Iter 14/100 - Loss: 48.512\n",
      "Iter 15/100 - Loss: 48.185\n",
      "Iter 16/100 - Loss: 47.862\n",
      "Iter 17/100 - Loss: 47.540\n",
      "Iter 18/100 - Loss: 47.222\n",
      "Iter 19/100 - Loss: 46.908\n",
      "Iter 20/100 - Loss: 46.599\n",
      "Iter 21/100 - Loss: 46.293\n",
      "Iter 22/100 - Loss: 45.989\n",
      "Iter 23/100 - Loss: 45.686\n",
      "Iter 24/100 - Loss: 45.386\n",
      "Iter 25/100 - Loss: 45.091\n",
      "Iter 26/100 - Loss: 44.798\n",
      "Iter 27/100 - Loss: 44.507\n",
      "Iter 28/100 - Loss: 44.217\n",
      "Iter 29/100 - Loss: 43.932\n",
      "Iter 30/100 - Loss: 43.650\n",
      "Iter 31/100 - Loss: 43.370\n",
      "Iter 32/100 - Loss: 43.091\n",
      "Iter 33/100 - Loss: 42.816\n",
      "Iter 34/100 - Loss: 42.545\n",
      "Iter 35/100 - Loss: 42.275\n",
      "Iter 36/100 - Loss: 42.009\n",
      "Iter 37/100 - Loss: 41.746\n",
      "Iter 38/100 - Loss: 41.485\n",
      "Iter 39/100 - Loss: 41.228\n",
      "Iter 40/100 - Loss: 40.973\n",
      "Iter 41/100 - Loss: 40.723\n",
      "Iter 42/100 - Loss: 40.476\n",
      "Iter 43/100 - Loss: 40.231\n",
      "Iter 44/100 - Loss: 39.990\n",
      "Iter 45/100 - Loss: 39.753\n",
      "Iter 46/100 - Loss: 39.519\n",
      "Iter 47/100 - Loss: 39.287\n",
      "Iter 48/100 - Loss: 39.060\n",
      "Iter 49/100 - Loss: 38.835\n",
      "Iter 50/100 - Loss: 38.615\n",
      "Iter 51/100 - Loss: 38.396\n",
      "Iter 52/100 - Loss: 38.182\n",
      "Iter 53/100 - Loss: 37.971\n",
      "Iter 54/100 - Loss: 37.762\n",
      "Iter 55/100 - Loss: 37.558\n",
      "Iter 56/100 - Loss: 37.356\n",
      "Iter 57/100 - Loss: 37.157\n",
      "Iter 58/100 - Loss: 36.962\n",
      "Iter 59/100 - Loss: 36.770\n",
      "Iter 60/100 - Loss: 36.580\n",
      "Iter 61/100 - Loss: 36.393\n",
      "Iter 62/100 - Loss: 36.209\n",
      "Iter 63/100 - Loss: 36.029\n",
      "Iter 64/100 - Loss: 35.851\n",
      "Iter 65/100 - Loss: 35.675\n",
      "Iter 66/100 - Loss: 35.503\n",
      "Iter 67/100 - Loss: 35.333\n",
      "Iter 68/100 - Loss: 35.166\n",
      "Iter 69/100 - Loss: 35.001\n",
      "Iter 70/100 - Loss: 34.839\n",
      "Iter 71/100 - Loss: 34.680\n",
      "Iter 72/100 - Loss: 34.522\n",
      "Iter 73/100 - Loss: 34.367\n",
      "Iter 74/100 - Loss: 34.214\n",
      "Iter 75/100 - Loss: 34.064\n",
      "Iter 76/100 - Loss: 33.916\n",
      "Iter 77/100 - Loss: 33.770\n",
      "Iter 78/100 - Loss: 33.626\n",
      "Iter 79/100 - Loss: 33.484\n",
      "Iter 80/100 - Loss: 33.345\n",
      "Iter 81/100 - Loss: 33.207\n",
      "Iter 82/100 - Loss: 33.071\n",
      "Iter 83/100 - Loss: 32.937\n",
      "Iter 84/100 - Loss: 32.805\n",
      "Iter 85/100 - Loss: 32.674\n",
      "Iter 86/100 - Loss: 32.546\n",
      "Iter 87/100 - Loss: 32.420\n",
      "Iter 88/100 - Loss: 32.294\n",
      "Iter 89/100 - Loss: 32.171\n",
      "Iter 90/100 - Loss: 32.050\n",
      "Iter 91/100 - Loss: 31.929\n",
      "Iter 92/100 - Loss: 31.811\n",
      "Iter 93/100 - Loss: 31.694\n",
      "Iter 94/100 - Loss: 31.579\n",
      "Iter 95/100 - Loss: 31.464\n",
      "Iter 96/100 - Loss: 31.352\n",
      "Iter 97/100 - Loss: 31.241\n",
      "Iter 98/100 - Loss: 31.131\n",
      "Iter 99/100 - Loss: 31.023\n",
      "Iter 100/100 - Loss: 30.916\n",
      "Test RMSE: 43.0180\n",
      "Test Mean Absolute Error: 31.6094\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Move  data to GPU\n",
    "if torch.cuda.is_available():\n",
    "    train_x = train_x.cuda()\n",
    "    train_y = train_y.cuda()\n",
    "    test_x  = test_x.cuda()\n",
    "    test_y  = test_y.cuda()\n",
    "    print(\"Data Moved to GPU\")\n",
    "\n",
    "\n",
    "\n",
    "kernel_types = ['RBF', 'Matern', 'Linear', 'RBF_Linear']#, 'Periodic', 'Matern_Linear', 'Mat52_Linear', 'RQ','RQ_Linear', 'Periodic_RBF']\n",
    "\n",
    "# Move the model to GPU\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "\n",
    "results = []\n",
    "for kernel_type in kernel_types:\n",
    "    print(f\"Training with kernel: {kernel_type}\")\n",
    "    kernel     = get_kernel(kernel_type= kernel_type).cuda()\n",
    "    model      = ExactGPModel(train_x, train_y, likelihood, kernel).cuda()\n",
    "\n",
    "    # Train\n",
    "    model, likelihood = train_model(model, likelihood, train_x, train_y, num_epochs=100)\n",
    "\n",
    "    # Test the model\n",
    "    pred_mean, pred_var, rmse, mean_abs_error = test_model(model, likelihood, test_x, test_y)\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test Mean Absolute Error: {mean_abs_error:.4f}\")\n",
    "    \n",
    "    # Save the model and metrics in a dictionary\n",
    "    model_metrics = {\n",
    "        'kernel': kernel_type,\n",
    "        'model': model.cpu(),\n",
    "        'likelihood': likelihood.cpu(),\n",
    "        'pred_mean': pred_mean,\n",
    "        'pred_var': pred_var,\n",
    "        'rmse': rmse,\n",
    "        'mean_abs_error': mean_abs_error\n",
    "    }\n",
    "    \n",
    "    # save dictionary to a list\n",
    "    \n",
    "    results.append(model_metrics)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f10cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550b7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7477cb",
   "metadata": {},
   "source": [
    "# Investiage Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d7d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb16228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75cdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8326355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: RBF\n",
      "RMSE: 45.9151\n",
      "Mean Absolute Error: 32.1825\n",
      "Predictions Mean: [122.06146 115.45743 140.46083 128.84917 102.67618]\n",
      "Predictions Variance: [9.694321  9.767435  9.0960865 8.135926  9.541519 ]\n",
      "\n",
      "\n",
      "Kernel: Matern\n",
      "RMSE: 43.0338\n",
      "Mean Absolute Error: 30.1534\n",
      "Predictions Mean: [140.82455 130.9536  148.05858 128.77681 117.52444]\n",
      "Predictions Variance: [16.492058 16.626595 15.824485 14.787728 16.41446 ]\n",
      "\n",
      "\n",
      "Kernel: Linear\n",
      "RMSE: 45.0057\n",
      "Mean Absolute Error: 34.2502\n",
      "Predictions Mean: [165.05795 124.40299 162.52916 153.25366 127.68254]\n",
      "Predictions Variance: [17.22464  17.218721 17.214495 17.210775 17.217068]\n",
      "\n",
      "\n",
      "Kernel: RBF_Linear\n",
      "RMSE: 43.0180\n",
      "Mean Absolute Error: 31.6094\n",
      "Predictions Mean: [154.1899  124.8269  152.50644 128.4646  122.1718 ]\n",
      "Predictions Variance: [34.58886  34.61811  34.40497  33.859146 34.533417]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(kernel_types)):\n",
    "    print(f\"Kernel: {results[i]['kernel']}\")\n",
    "    print(f\"RMSE: {results[i]['rmse']:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {results[i]['mean_abs_error']:.4f}\")\n",
    "    print(f\"Predictions Mean: {results[i]['pred_mean'][:5]}\")\n",
    "    print(f\"Predictions Variance: {results[i]['pred_var'][:5]}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
