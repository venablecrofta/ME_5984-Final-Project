{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f7a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076f824",
   "metadata": {},
   "source": [
    "# Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00977032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the train csv data set via numpy\n",
    "def load_data(file_path):\n",
    "    \n",
    "    \"\"\"Load data from a CSV file into a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "    data = pd.read_csv(file_path, delimiter=' ', header=None)\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_histgrams(df, display_corr_matrix=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot histograms of the DataFrame columns and optionally display a correlation matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    df.hist(bins=30, figsize=(8,6), color='b')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if display_corr_matrix:\n",
    "        corr_matrix = df.corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.matshow(corr_matrix, fignum=1)\n",
    "        plt.colorbar()\n",
    "        plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n",
    "        plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "        plt.title('Correlation Matrix', pad=20)\n",
    "        plt.show()\n",
    "    \n",
    "def delete_bad_sensors(df):\n",
    "    \n",
    "    \"\"\"Identify and remove sensors with NaN or zero correlation in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with bad sensors removed.\n",
    "        list: List of labels of the removed sensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the correlation matrix from columns 1 through -2 of train_df\n",
    "    correlation_matrix = df.iloc[:, 1:-1].corr()\n",
    "\n",
    "    # Select the 2nd column of that matrix (0-based index 1)\n",
    "    col = correlation_matrix.iloc[:, 1]\n",
    "\n",
    "    # Build your mask: NaNs or exact zeros\n",
    "    mask = col.isna() | (col == 0)\n",
    "\n",
    "    # 1) If you want the actual **column names** (labels) whose correlation is bad:\n",
    "    bad_labels = col.index[mask].tolist()\n",
    "    print(\"Columns with NaN or 0 correlation:\", bad_labels)\n",
    "\n",
    "    # 2) If you instead want their **integer positions** within this correlation matrix:\n",
    "    bad_positions = np.where(mask)[0]         # 0-based positions\n",
    "    bad_positions_one_based = bad_positions + 1\n",
    "    print(\"Bad Sensors 1-based positions:\", bad_positions_one_based)\n",
    "\n",
    "    # Drop the bad sensors from the training data\n",
    "    df.drop(columns=bad_labels, inplace=True)\n",
    "    \n",
    "    return df, bad_labels\n",
    "\n",
    "def normalize_data(train_df):\n",
    "    \n",
    "    \"\"\"Normalize the training data using Min-Max scaling.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Normalized DataFrame with values scaled to the range [-1, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features=train_df.columns\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    train_df[features] = min_max_scaler.fit_transform(train_df[features])\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "# Function to calculate the remaining flights for each engine\n",
    "def calculate_remaining_flights(data):\n",
    "    # Group by engine ID and find the maximum cycle for each engine\n",
    "    max_cycles = data.groupby(0)[1].max().reset_index()\n",
    "    max_cycles.columns = [0, 'max_cycle']\n",
    "    \n",
    "    # Merge to get the remaining flights\n",
    "    data = data.merge(max_cycles, on=0)\n",
    "    data['remaining_flights'] = data['max_cycle'] - data[1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442971a7",
   "metadata": {},
   "source": [
    "# Gaussian Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bb5bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gpflow as gpf\n",
    "from gpflow.utilities import print_summary\n",
    "\n",
    "def get_gpus():\n",
    "    \"\"\"\n",
    "    Returns a list of available GPU devices.\n",
    "    \"\"\"\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"GPUs:\", gpus)\n",
    "    return gpus\n",
    "\n",
    "\n",
    "def additive_kernel():\n",
    "    \"\"\"Create an additive kernel composed of a linear and RBF kernel.\n",
    "    \"\"\"\n",
    "    linear_kernel = gpf.kernels.Linear()\n",
    "    rbf_kernel = gpf.kernels.RBF()\n",
    "    additive_kernel = linear_kernel + rbf_kernel\n",
    "    \n",
    "    return additive_kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(train_df, targets_df, num_points=1000):\n",
    "    \"\"\"\n",
    "    Extracts and formats training inputs and targets from DataFrames.\n",
    "    - train_df: pandas.DataFrame of features\n",
    "    - targets_df: pandas.DataFrame with a 'remaining_flights' column\n",
    "    - num_points: number of rows to select\n",
    "    Returns: (train_x, train_y) as float64 numpy arrays with shape [N, D] and [N, 1]\n",
    "    \"\"\"\n",
    "    x = train_df.to_numpy().astype(np.float64)[:num_points, :]\n",
    "    y = targets_df['remaining_flights'].to_numpy().astype(np.float64)[:num_points]\n",
    "\n",
    "    # Ensure 2D arrays\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "    if y.ndim == 1:\n",
    "        y = y[:, None]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def build_gpr_model(train_x, train_y, kernel=gpf.kernels.RBF() ,noise_variance=1e-2):\n",
    "    \"\"\"\n",
    "    Builds and returns a GPFlow GPR model with an additive linear + RBF kernel.\n",
    "    Also initializes the likelihood variance.\n",
    "    Returns: (model)\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_func = gpf.mean_functions.Constant()\n",
    "\n",
    "    model = gpf.models.GPR(data=(train_x, train_y), kernel=kernel, mean_function=mean_func)\n",
    "    model.likelihood.variance.assign(noise_variance)\n",
    "    return model\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def optimization_step(model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model.training_loss()\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Trains the GPFlow model using the Adam optimizer.\n",
    "    Returns: (model)\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = optimization_step(model, optimizer)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "    \n",
    "    print_summary(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# model = build_gpr_model(train_x, train_y, kernel=kernel)\n",
    "\n",
    "# # List all trainable variables:\n",
    "# for v in model.trainable_variables:\n",
    "#     print(v.name, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71601533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN or 0 correlation: [4, 5, 9, 14, 20, 22, 23]\n",
      "Bad Sensors 1-based positions: [ 4  5  9 14 20 22 23]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m train_sensors \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Normalize the sensor data\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m train_sensors \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Prepare the targets DataFrame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m targets_df \u001b[38;5;241m=\u001b[39m train_df[[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremaining_flights\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[1;32mIn[2], line 80\u001b[0m, in \u001b[0;36mnormalize_data\u001b[1;34m(train_df)\u001b[0m\n\u001b[0;32m     78\u001b[0m features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(train_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     79\u001b[0m min_max_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 80\u001b[0m train_df[features] \u001b[38;5;241m=\u001b[39m \u001b[43mmin_max_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_df\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:447\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:487\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    484\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    486\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    496\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2836\u001b[0m     _estimator,\n\u001b[0;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2844\u001b[0m ):\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[0;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\utils\\validation.py:2715\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2688\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set or check the `feature_names_in_` attribute of an estimator.\u001b[39;00m\n\u001b[0;32m   2689\u001b[0m \n\u001b[0;32m   2690\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2711\u001b[0m \u001b[38;5;124;03m       should set `reset=False`.\u001b[39;00m\n\u001b[0;32m   2712\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[1;32m-> 2715\u001b[0m     feature_names_in \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2717\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m feature_names_in\n",
      "File \u001b[1;32mc:\\Users\\lindy\\anaconda3\\envs\\MLMEProj\\lib\\site-packages\\sklearn\\utils\\validation.py:2417\u001b[0m, in \u001b[0;36m_get_feature_names\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m   2415\u001b[0m \u001b[38;5;66;03m# mixed type of string and non-string is not supported\u001b[39;00m\n\u001b[0;32m   2416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m-> 2417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names are only supported if all input features have string names, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut your input has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as feature name / column name types. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want feature names to be stored and validated, you must convert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem all to strings, by using X.columns = X.columns.astype(str) for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample. Otherwise you can remove feature / column names from your input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata, or convert them all to a non-string data type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2424\u001b[0m     )\n\u001b[0;32m   2426\u001b[0m \u001b[38;5;66;03m# Only feature names of all strings are supported\u001b[39;00m\n\u001b[0;32m   2427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "source": [
    "train_df = load_data('train_FD001.csv')\n",
    "\n",
    "train_df.head()\n",
    "train_df.drop(labels=[26, 27], axis=1, inplace=True)\n",
    "\n",
    "# Remove the bad sensors\n",
    "train_df, bad_sensors = delete_bad_sensors(train_df)\n",
    "# Calculate remaining flights\n",
    "train_df = calculate_remaining_flights(train_df)\n",
    "\n",
    "# Select just the sensor data\n",
    "train_sensors = train_df.iloc[:, 2:-1]\n",
    "# Normalize the sensor data\n",
    "train_sensors = normalize_data(train_sensors)\n",
    "\n",
    "# Prepare the targets DataFrame\n",
    "targets_df = train_df[[0, 1, 'remaining_flights']].copy()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLMEProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
